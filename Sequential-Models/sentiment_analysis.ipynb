{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO2fdMjUksRer8EbRzOqFSy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cpt-Shaan/deep-stuff/blob/main/Sequential-Models/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis on IMDB dataset using single layer LSTMs."
      ],
      "metadata": {
        "id": "KuSPEpvSv0Pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting Device"
      ],
      "metadata": {
        "id": "cNgHvFQTCL7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "VPWPHMTMIpe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading CSV File and storing reviews and labels."
      ],
      "metadata": {
        "id": "1xMur8ZbCBIV"
      }
    },
    {
      "source": [
        "import csv\n",
        "\n",
        "reviews = []\n",
        "labels = []\n",
        "with open(\"IMDB Dataset.csv\", \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)  # Skip the header row\n",
        "    for row in reader:\n",
        "        # Check if the row has at least 2 elements before accessing the second element\n",
        "        if len(row) > 1:\n",
        "            reviews.append(row[0])\n",
        "            labels.append(row[1])\n",
        "        else:\n",
        "            # Handle rows with missing data, for example by skipping them or adding a default value\n",
        "            print(f\"Skipping row: {row}\") # Print a warning message for the skipped row"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trv5b1wzzupW",
        "outputId": "bfdd02aa-29d4-461c-b210-7fd32cc09b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping row: [\"I think that the basic idea of any movie is to entertain or to inform. If you want information you are looking at true life movies and historical movies. Sometimes these are one of the same. The other side of the coin is to entertain. Did Hitch entertain me? Yes it did. Okay the formula is standard. Boy meets girl or in this case boys met girls. They get together have a falling out then get back together. However the way it happened in this movie was refreshing. I particularly liked the bar scene with Hitch and Sara. The Allegra Albert romance was a delight to watch unfold, most REAL men are shy when it comes to wooing the woman of their dreams and had I had Hitch's advice I would probably have got my wife up the altar in half the time.I read the first comment on this film that appeared to suggest that this movie was played safely and good have had a few more laughs. I tend to disagree there are so many laughs you can pack into a romantic comedy without turning it into a farce. Besides relationships have ther\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(reviews))\n",
        "print(len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZPFWN-Dxux9",
        "outputId": "4343ebc8-febc-42bb-dc1f-a7fbee7e3fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14209\n",
            "14209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation"
      ],
      "metadata": {
        "id": "g7XYkY7Yz8Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Punctuations from the text as pre-processing"
      ],
      "metadata": {
        "id": "pvRmKj7qCTw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = \"\".join([char for char in text if char not in punctuation])\n",
        "    return text\n",
        "\n",
        "for review in reviews:\n",
        "    preprocess_text(review)\n"
      ],
      "metadata": {
        "id": "VfEpUDh30KW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Providing 2-way lookup methods for words and their IDs which are assigned here."
      ],
      "metadata": {
        "id": "YD6cIFt3CaCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_words = []\n",
        "for review in reviews:\n",
        "    all_words += review.split()\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "word_list = sorted(word_counts, key = word_counts.get, reverse = True)\n",
        "vocab_to_int = {word:idx+1 for idx, word in enumerate(word_list)}\n",
        "int_to_vocab = {idx:word for word, idx in vocab_to_int.items()}\n",
        "\n",
        "encoded_reviews = [[vocab_to_int.get(word) for word in review.split()] for review in reviews]"
      ],
      "metadata": {
        "id": "ljcbVp4q0c3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_labels = [1 if label == 'positive' else 0 for label in labels]\n",
        "print(len(encoded_reviews))\n",
        "print(len(encoded_labels))"
      ],
      "metadata": {
        "id": "HdNnqKdL1j5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f13901-2e53-4f5b-dfe4-e69a94e9f9e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14209\n",
            "14209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding Text Sequence to maintain same length for each sentence / review."
      ],
      "metadata": {
        "id": "rv2mybeACl5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "encoded_labels = np.array( [label for idx, label in enumerate(encoded_labels) if len(encoded_reviews[idx]) > 0] )\n",
        "encoded_reviews = [review for review in encoded_reviews if len(review) > 0]\n",
        "\n",
        "def pad_text(encoded_reviews, seq_length):\n",
        "\n",
        "    reviews = []\n",
        "\n",
        "    for review in encoded_reviews:\n",
        "        if len(review) >= seq_length:\n",
        "            reviews.append(review[:seq_length])\n",
        "        else:\n",
        "            reviews.append([0]*(seq_length-len(review)) + review)\n",
        "\n",
        "    return np.array(reviews)\n",
        "\n",
        "\n",
        "padded_reviews = pad_text(encoded_reviews, seq_length = 200)\n",
        "# Only Considering 1st 200 words of each review."
      ],
      "metadata": {
        "id": "Sg3S_AnB2jHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting Training, Validation and Testing dataset."
      ],
      "metadata": {
        "id": "IRTmISJjCuWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.8\n",
        "valid_ratio = (1 - train_ratio)/2\n",
        "total = padded_reviews.shape[0]\n",
        "train_cutoff = int(total * train_ratio)\n",
        "valid_cutoff = int(total * (1 - valid_ratio))\n",
        "\n",
        "train_x, train_y = padded_reviews[:train_cutoff], encoded_labels[:train_cutoff]\n",
        "valid_x, valid_y = padded_reviews[train_cutoff : valid_cutoff], encoded_labels[train_cutoff : valid_cutoff]\n",
        "test_x, test_y = padded_reviews[valid_cutoff:], encoded_labels[valid_cutoff:]\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_x = torch.tensor(train_x)\n",
        "train_y = torch.tensor(train_y)\n",
        "valid_x = torch.tensor(valid_x)\n",
        "valid_y = torch.tensor(valid_y)\n",
        "test_x = torch.tensor(test_x)\n",
        "test_y = torch.tensor(test_y)\n",
        "\n",
        "train_data = TensorDataset(train_x, train_y)\n",
        "valid_data = TensorDataset(valid_x, valid_y)\n",
        "test_data = TensorDataset(test_x, test_y)\n",
        "\n",
        "batch_size = 50\n",
        "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
        "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n",
        "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)\n"
      ],
      "metadata": {
        "id": "28eKGbpg6ADd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MOdel Architecture using LSTM cells."
      ],
      "metadata": {
        "id": "crvQUisuC3uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_vocab = n_vocab\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
        "        self.rnn = nn.RNN(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "        self.fc = nn.Linear(n_hidden, n_output)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward (self, input_words):\n",
        "\n",
        "        embedded_words = self.embedding(input_words)\n",
        "        rnn_out, h = self.lstm(embedded_words)\n",
        "        rnn_out = self.dropout(rnn_out)\n",
        "        rnn_out = rnn_out.contiguous().view(-1, self.n_hidden)\n",
        "        fc_out = self.fc(rnn_out)\n",
        "        sigmoid_out = self.sigmoid(fc_out)\n",
        "        sigmoid_out = sigmoid_out.view(batch_size, -1)\n",
        "\n",
        "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
        "        sigmoid_last = sigmoid_out[:, -1]\n",
        "\n",
        "        return sigmoid_last, h\n",
        "\n",
        "\n",
        "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
        "\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        weights = next(self.parameters()).data\n",
        "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "\n",
        "        return h"
      ],
      "metadata": {
        "id": "-LWipiKyHPJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing Hyperparameters"
      ],
      "metadata": {
        "id": "vVHlmCxfFdIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_vocab = len(vocab_to_int)\n",
        "n_embed = 400\n",
        "n_hidden = 512\n",
        "n_output = 1\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentRNN(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "_qA8MOsKHVQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Process using Adam's optimizer"
      ],
      "metadata": {
        "id": "3qRPwej0FiX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr = 0.0001)\n",
        "\n",
        "print_every = 50\n",
        "step = 0\n",
        "n_epochs = 50\n",
        "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    h = net.init_hidden(batch_size)\n",
        "    net.train()\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        step += 1\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # making requires_grad = False for the latest set of h\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        net.zero_grad()\n",
        "        output, h = net(inputs)\n",
        "\n",
        "        # Adjust output and labels to have the same size for the last batch\n",
        "        # by taking only the relevant elements from the output tensor\n",
        "        output = output[:labels.size(0)]\n",
        "\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        if (step % print_every) == 0:\n",
        "            net.eval()\n",
        "            valid_losses = []\n",
        "            v_h = net.init_hidden(batch_size)\n",
        "\n",
        "            for v_inputs, v_labels in valid_loader:\n",
        "                v_inputs, v_labels = v_inputs.to(device), v_labels.to(device) # Use v_inputs and v_labels instead of inputs and labels\n",
        "\n",
        "                v_h = tuple([each.data for each in v_h])\n",
        "\n",
        "                v_output, v_h = net(v_inputs)\n",
        "\n",
        "                # Adjust output and labels to have the same size for the last batch in validation as well\n",
        "                v_output = v_output[:v_labels.size(0)]\n",
        "\n",
        "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
        "                valid_losses.append(v_loss.item())\n",
        "\n",
        "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
        "                  \"Step: {}\".format(step),\n",
        "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
        "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
        "            net.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekjnoJlyJKWD",
        "outputId": "24835f4e-f5c0-4f69-e2f3-8335bbbf6a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-2219a3b2c924>:31: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
            "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/50 Step: 50 Training Loss: 0.7265 Validation Loss: 0.6948\n",
            "Epoch: 1/50 Step: 100 Training Loss: 0.6879 Validation Loss: 0.6941\n",
            "Epoch: 1/50 Step: 150 Training Loss: 0.7156 Validation Loss: 0.6931\n",
            "Epoch: 1/50 Step: 200 Training Loss: 0.7111 Validation Loss: 0.6937\n",
            "Epoch: 2/50 Step: 250 Training Loss: 0.6982 Validation Loss: 0.6932\n",
            "Epoch: 2/50 Step: 300 Training Loss: 0.7115 Validation Loss: 0.6926\n",
            "Epoch: 2/50 Step: 350 Training Loss: 0.7121 Validation Loss: 0.6941\n",
            "Epoch: 2/50 Step: 400 Training Loss: 0.7187 Validation Loss: 0.6948\n",
            "Epoch: 2/50 Step: 450 Training Loss: 0.6746 Validation Loss: 0.6959\n",
            "Epoch: 3/50 Step: 500 Training Loss: 0.7055 Validation Loss: 0.6931\n",
            "Epoch: 3/50 Step: 550 Training Loss: 0.6900 Validation Loss: 0.6935\n",
            "Epoch: 3/50 Step: 600 Training Loss: 0.6898 Validation Loss: 0.6936\n",
            "Epoch: 3/50 Step: 650 Training Loss: 0.7060 Validation Loss: 0.6959\n",
            "Epoch: 4/50 Step: 700 Training Loss: 0.6955 Validation Loss: 0.6941\n",
            "Epoch: 4/50 Step: 750 Training Loss: 0.6868 Validation Loss: 0.6948\n",
            "Epoch: 4/50 Step: 800 Training Loss: 0.7109 Validation Loss: 0.6946\n",
            "Epoch: 4/50 Step: 850 Training Loss: 0.6890 Validation Loss: 0.6944\n",
            "Epoch: 4/50 Step: 900 Training Loss: 0.7091 Validation Loss: 0.6962\n",
            "Epoch: 5/50 Step: 950 Training Loss: 0.7054 Validation Loss: 0.6967\n",
            "Epoch: 5/50 Step: 1000 Training Loss: 0.6779 Validation Loss: 0.6942\n",
            "Epoch: 5/50 Step: 1050 Training Loss: 0.7025 Validation Loss: 0.6938\n",
            "Epoch: 5/50 Step: 1100 Training Loss: 0.6929 Validation Loss: 0.6940\n",
            "Epoch: 6/50 Step: 1150 Training Loss: 0.6759 Validation Loss: 0.6940\n",
            "Epoch: 6/50 Step: 1200 Training Loss: 0.7110 Validation Loss: 0.6934\n",
            "Epoch: 6/50 Step: 1250 Training Loss: 0.6856 Validation Loss: 0.6948\n",
            "Epoch: 6/50 Step: 1300 Training Loss: 0.6980 Validation Loss: 0.6901\n",
            "Epoch: 6/50 Step: 1350 Training Loss: 0.6903 Validation Loss: 0.6904\n",
            "Epoch: 7/50 Step: 1400 Training Loss: 0.7131 Validation Loss: 0.6967\n",
            "Epoch: 7/50 Step: 1450 Training Loss: 0.6833 Validation Loss: 0.6958\n",
            "Epoch: 7/50 Step: 1500 Training Loss: 0.7185 Validation Loss: 0.6961\n",
            "Epoch: 7/50 Step: 1550 Training Loss: 0.7084 Validation Loss: 0.6966\n",
            "Epoch: 8/50 Step: 1600 Training Loss: 0.6927 Validation Loss: 0.6941\n",
            "Epoch: 8/50 Step: 1650 Training Loss: 0.6921 Validation Loss: 0.6950\n",
            "Epoch: 8/50 Step: 1700 Training Loss: 0.7066 Validation Loss: 0.6964\n",
            "Epoch: 8/50 Step: 1750 Training Loss: 0.6711 Validation Loss: 0.6953\n",
            "Epoch: 8/50 Step: 1800 Training Loss: 0.6713 Validation Loss: 0.6975\n",
            "Epoch: 9/50 Step: 1850 Training Loss: 0.6836 Validation Loss: 0.6955\n",
            "Epoch: 9/50 Step: 1900 Training Loss: 0.6957 Validation Loss: 0.6967\n",
            "Epoch: 9/50 Step: 1950 Training Loss: 0.6734 Validation Loss: 0.6959\n",
            "Epoch: 9/50 Step: 2000 Training Loss: 0.6835 Validation Loss: 0.6962\n",
            "Epoch: 9/50 Step: 2050 Training Loss: 0.6897 Validation Loss: 0.6933\n",
            "Epoch: 10/50 Step: 2100 Training Loss: 0.7008 Validation Loss: 0.6934\n",
            "Epoch: 10/50 Step: 2150 Training Loss: 0.6537 Validation Loss: 0.6950\n",
            "Epoch: 10/50 Step: 2200 Training Loss: 0.6984 Validation Loss: 0.6934\n",
            "Epoch: 10/50 Step: 2250 Training Loss: 0.6813 Validation Loss: 0.6928\n",
            "Epoch: 11/50 Step: 2300 Training Loss: 0.6326 Validation Loss: 0.6840\n",
            "Epoch: 11/50 Step: 2350 Training Loss: 0.5724 Validation Loss: 0.6859\n",
            "Epoch: 11/50 Step: 2400 Training Loss: 0.5876 Validation Loss: 0.6747\n",
            "Epoch: 11/50 Step: 2450 Training Loss: 0.5314 Validation Loss: 0.6571\n",
            "Epoch: 11/50 Step: 2500 Training Loss: 0.5896 Validation Loss: 0.6827\n",
            "Epoch: 12/50 Step: 2550 Training Loss: 0.6237 Validation Loss: 0.6644\n",
            "Epoch: 12/50 Step: 2600 Training Loss: 0.5684 Validation Loss: 0.6718\n",
            "Epoch: 12/50 Step: 2650 Training Loss: 0.5393 Validation Loss: 0.6819\n",
            "Epoch: 12/50 Step: 2700 Training Loss: 0.6527 Validation Loss: 0.6733\n",
            "Epoch: 13/50 Step: 2750 Training Loss: 0.5124 Validation Loss: 0.6756\n",
            "Epoch: 13/50 Step: 2800 Training Loss: 0.6153 Validation Loss: 0.7097\n",
            "Epoch: 13/50 Step: 2850 Training Loss: 0.5326 Validation Loss: 0.6721\n",
            "Epoch: 13/50 Step: 2900 Training Loss: 0.5115 Validation Loss: 0.6792\n",
            "Epoch: 13/50 Step: 2950 Training Loss: 0.4043 Validation Loss: 0.6736\n",
            "Epoch: 14/50 Step: 3000 Training Loss: 0.5152 Validation Loss: 0.7028\n",
            "Epoch: 14/50 Step: 3050 Training Loss: 0.5378 Validation Loss: 0.6621\n",
            "Epoch: 14/50 Step: 3100 Training Loss: 0.4087 Validation Loss: 0.7088\n",
            "Epoch: 14/50 Step: 3150 Training Loss: 0.4193 Validation Loss: 0.6746\n",
            "Epoch: 15/50 Step: 3200 Training Loss: 0.5295 Validation Loss: 0.6850\n",
            "Epoch: 15/50 Step: 3250 Training Loss: 0.2896 Validation Loss: 0.7100\n",
            "Epoch: 15/50 Step: 3300 Training Loss: 0.4584 Validation Loss: 0.6839\n",
            "Epoch: 15/50 Step: 3350 Training Loss: 0.4069 Validation Loss: 0.6804\n",
            "Epoch: 15/50 Step: 3400 Training Loss: 0.4218 Validation Loss: 0.6850\n",
            "Epoch: 16/50 Step: 3450 Training Loss: 0.4025 Validation Loss: 0.6996\n",
            "Epoch: 16/50 Step: 3500 Training Loss: 0.5693 Validation Loss: 0.7418\n",
            "Epoch: 16/50 Step: 3550 Training Loss: 0.4870 Validation Loss: 0.7317\n",
            "Epoch: 16/50 Step: 3600 Training Loss: 0.4966 Validation Loss: 0.6971\n",
            "Epoch: 17/50 Step: 3650 Training Loss: 0.3904 Validation Loss: 0.7033\n",
            "Epoch: 17/50 Step: 3700 Training Loss: 0.4335 Validation Loss: 0.7121\n",
            "Epoch: 17/50 Step: 3750 Training Loss: 0.4512 Validation Loss: 0.7450\n",
            "Epoch: 17/50 Step: 3800 Training Loss: 0.2866 Validation Loss: 0.7423\n",
            "Epoch: 17/50 Step: 3850 Training Loss: 0.6006 Validation Loss: 0.7299\n",
            "Epoch: 18/50 Step: 3900 Training Loss: 0.5807 Validation Loss: 0.7384\n",
            "Epoch: 18/50 Step: 3950 Training Loss: 0.5044 Validation Loss: 0.7464\n",
            "Epoch: 18/50 Step: 4000 Training Loss: 0.6644 Validation Loss: 0.7528\n",
            "Epoch: 18/50 Step: 4050 Training Loss: 0.3176 Validation Loss: 0.7690\n",
            "Epoch: 18/50 Step: 4100 Training Loss: 0.3339 Validation Loss: 0.7525\n",
            "Epoch: 19/50 Step: 4150 Training Loss: 0.4222 Validation Loss: 0.7569\n",
            "Epoch: 19/50 Step: 4200 Training Loss: 0.4294 Validation Loss: 0.7962\n",
            "Epoch: 19/50 Step: 4250 Training Loss: 0.2588 Validation Loss: 0.7720\n",
            "Epoch: 19/50 Step: 4300 Training Loss: 0.4158 Validation Loss: 0.7591\n",
            "Epoch: 20/50 Step: 4350 Training Loss: 0.2004 Validation Loss: 0.7787\n",
            "Epoch: 20/50 Step: 4400 Training Loss: 0.3614 Validation Loss: 0.8027\n",
            "Epoch: 20/50 Step: 4450 Training Loss: 0.4074 Validation Loss: 0.8207\n",
            "Epoch: 20/50 Step: 4500 Training Loss: 0.4298 Validation Loss: 0.7899\n",
            "Epoch: 20/50 Step: 4550 Training Loss: 0.4071 Validation Loss: 0.7758\n",
            "Epoch: 21/50 Step: 4600 Training Loss: 0.4113 Validation Loss: 0.8149\n",
            "Epoch: 21/50 Step: 4650 Training Loss: 0.3496 Validation Loss: 0.8356\n",
            "Epoch: 21/50 Step: 4700 Training Loss: 0.2733 Validation Loss: 0.8214\n",
            "Epoch: 21/50 Step: 4750 Training Loss: 0.3972 Validation Loss: 0.8749\n",
            "Epoch: 22/50 Step: 4800 Training Loss: 0.3403 Validation Loss: 0.8285\n",
            "Epoch: 22/50 Step: 4850 Training Loss: 0.2119 Validation Loss: 0.8427\n",
            "Epoch: 22/50 Step: 4900 Training Loss: 0.4100 Validation Loss: 0.8907\n",
            "Epoch: 22/50 Step: 4950 Training Loss: 0.3003 Validation Loss: 0.9023\n",
            "Epoch: 22/50 Step: 5000 Training Loss: 0.3766 Validation Loss: 0.8778\n",
            "Epoch: 23/50 Step: 5050 Training Loss: 0.1946 Validation Loss: 0.8869\n",
            "Epoch: 23/50 Step: 5100 Training Loss: 0.2320 Validation Loss: 0.9010\n",
            "Epoch: 23/50 Step: 5150 Training Loss: 0.3036 Validation Loss: 0.8795\n",
            "Epoch: 23/50 Step: 5200 Training Loss: 0.1977 Validation Loss: 0.8532\n",
            "Epoch: 24/50 Step: 5250 Training Loss: 0.2945 Validation Loss: 0.8478\n",
            "Epoch: 24/50 Step: 5300 Training Loss: 0.3949 Validation Loss: 0.8900\n",
            "Epoch: 24/50 Step: 5350 Training Loss: 0.1577 Validation Loss: 0.9051\n",
            "Epoch: 24/50 Step: 5400 Training Loss: 0.2349 Validation Loss: 0.9096\n",
            "Epoch: 24/50 Step: 5450 Training Loss: 0.2133 Validation Loss: 0.9209\n",
            "Epoch: 25/50 Step: 5500 Training Loss: 0.1686 Validation Loss: 0.9001\n",
            "Epoch: 25/50 Step: 5550 Training Loss: 0.2893 Validation Loss: 0.9233\n",
            "Epoch: 25/50 Step: 5600 Training Loss: 0.2331 Validation Loss: 0.9336\n",
            "Epoch: 25/50 Step: 5650 Training Loss: 0.2063 Validation Loss: 0.9551\n",
            "Epoch: 25/50 Step: 5700 Training Loss: 1.5960 Validation Loss: 0.9261\n",
            "Epoch: 26/50 Step: 5750 Training Loss: 0.1655 Validation Loss: 0.9642\n",
            "Epoch: 26/50 Step: 5800 Training Loss: 0.2943 Validation Loss: 0.9314\n",
            "Epoch: 26/50 Step: 5850 Training Loss: 0.1429 Validation Loss: 0.9314\n",
            "Epoch: 26/50 Step: 5900 Training Loss: 0.2183 Validation Loss: 0.9268\n",
            "Epoch: 27/50 Step: 5950 Training Loss: 0.1811 Validation Loss: 0.9337\n",
            "Epoch: 27/50 Step: 6000 Training Loss: 0.1427 Validation Loss: 0.9582\n",
            "Epoch: 27/50 Step: 6050 Training Loss: 0.2051 Validation Loss: 0.9524\n",
            "Epoch: 27/50 Step: 6100 Training Loss: 0.2499 Validation Loss: 0.9647\n",
            "Epoch: 27/50 Step: 6150 Training Loss: 0.2642 Validation Loss: 0.9756\n",
            "Epoch: 28/50 Step: 6200 Training Loss: 0.1858 Validation Loss: 0.9728\n",
            "Epoch: 28/50 Step: 6250 Training Loss: 0.2383 Validation Loss: 1.0062\n",
            "Epoch: 28/50 Step: 6300 Training Loss: 0.1116 Validation Loss: 0.9812\n",
            "Epoch: 28/50 Step: 6350 Training Loss: 0.0930 Validation Loss: 1.0003\n",
            "Epoch: 29/50 Step: 6400 Training Loss: 0.1509 Validation Loss: 0.9863\n",
            "Epoch: 29/50 Step: 6450 Training Loss: 0.2588 Validation Loss: 0.9888\n",
            "Epoch: 29/50 Step: 6500 Training Loss: 0.3192 Validation Loss: 1.0024\n",
            "Epoch: 29/50 Step: 6550 Training Loss: 0.0748 Validation Loss: 1.0076\n",
            "Epoch: 29/50 Step: 6600 Training Loss: 0.2914 Validation Loss: 0.9925\n",
            "Epoch: 30/50 Step: 6650 Training Loss: 0.1109 Validation Loss: 0.9889\n",
            "Epoch: 30/50 Step: 6700 Training Loss: 0.1081 Validation Loss: 1.0580\n",
            "Epoch: 30/50 Step: 6750 Training Loss: 0.1729 Validation Loss: 1.0748\n",
            "Epoch: 30/50 Step: 6800 Training Loss: 0.1087 Validation Loss: 1.0992\n",
            "Epoch: 31/50 Step: 6850 Training Loss: 0.4032 Validation Loss: 1.0323\n",
            "Epoch: 31/50 Step: 6900 Training Loss: 0.0892 Validation Loss: 1.0412\n",
            "Epoch: 31/50 Step: 6950 Training Loss: 0.1686 Validation Loss: 1.0037\n",
            "Epoch: 31/50 Step: 7000 Training Loss: 0.1347 Validation Loss: 1.0806\n",
            "Epoch: 31/50 Step: 7050 Training Loss: 0.0733 Validation Loss: 1.0917\n",
            "Epoch: 32/50 Step: 7100 Training Loss: 0.3008 Validation Loss: 1.0866\n",
            "Epoch: 32/50 Step: 7150 Training Loss: 0.2508 Validation Loss: 1.0985\n",
            "Epoch: 32/50 Step: 7200 Training Loss: 0.0560 Validation Loss: 1.1103\n",
            "Epoch: 32/50 Step: 7250 Training Loss: 0.1424 Validation Loss: 1.0929\n",
            "Epoch: 33/50 Step: 7300 Training Loss: 0.2189 Validation Loss: 1.0993\n",
            "Epoch: 33/50 Step: 7350 Training Loss: 0.0907 Validation Loss: 1.0935\n",
            "Epoch: 33/50 Step: 7400 Training Loss: 0.1079 Validation Loss: 1.0789\n",
            "Epoch: 33/50 Step: 7450 Training Loss: 0.1240 Validation Loss: 1.0534\n",
            "Epoch: 33/50 Step: 7500 Training Loss: 0.1038 Validation Loss: 1.0907\n",
            "Epoch: 34/50 Step: 7550 Training Loss: 0.0543 Validation Loss: 1.0957\n",
            "Epoch: 34/50 Step: 7600 Training Loss: 0.0234 Validation Loss: 1.1381\n",
            "Epoch: 34/50 Step: 7650 Training Loss: 0.2057 Validation Loss: 1.1104\n",
            "Epoch: 34/50 Step: 7700 Training Loss: 0.1643 Validation Loss: 1.1480\n",
            "Epoch: 34/50 Step: 7750 Training Loss: 0.0669 Validation Loss: 1.1070\n",
            "Epoch: 35/50 Step: 7800 Training Loss: 0.1517 Validation Loss: 1.1283\n",
            "Epoch: 35/50 Step: 7850 Training Loss: 0.1625 Validation Loss: 1.1718\n",
            "Epoch: 35/50 Step: 7900 Training Loss: 0.0443 Validation Loss: 1.1834\n",
            "Epoch: 35/50 Step: 7950 Training Loss: 0.1716 Validation Loss: 1.1557\n",
            "Epoch: 36/50 Step: 8000 Training Loss: 0.0235 Validation Loss: 1.0966\n",
            "Epoch: 36/50 Step: 8050 Training Loss: 0.1334 Validation Loss: 1.1375\n",
            "Epoch: 36/50 Step: 8100 Training Loss: 0.0199 Validation Loss: 1.1659\n",
            "Epoch: 36/50 Step: 8150 Training Loss: 0.1307 Validation Loss: 1.1656\n",
            "Epoch: 36/50 Step: 8200 Training Loss: 0.1524 Validation Loss: 1.1708\n",
            "Epoch: 37/50 Step: 8250 Training Loss: 0.0472 Validation Loss: 1.1660\n",
            "Epoch: 37/50 Step: 8300 Training Loss: 0.0384 Validation Loss: 1.2361\n",
            "Epoch: 37/50 Step: 8350 Training Loss: 0.0310 Validation Loss: 1.2106\n",
            "Epoch: 37/50 Step: 8400 Training Loss: 0.1037 Validation Loss: 1.2177\n",
            "Epoch: 38/50 Step: 8450 Training Loss: 0.0470 Validation Loss: 1.2141\n",
            "Epoch: 38/50 Step: 8500 Training Loss: 0.0803 Validation Loss: 1.2100\n",
            "Epoch: 38/50 Step: 8550 Training Loss: 0.0378 Validation Loss: 1.2525\n",
            "Epoch: 38/50 Step: 8600 Training Loss: 0.0330 Validation Loss: 1.2991\n",
            "Epoch: 38/50 Step: 8650 Training Loss: 0.0192 Validation Loss: 1.1925\n",
            "Epoch: 39/50 Step: 8700 Training Loss: 0.1475 Validation Loss: 1.2434\n",
            "Epoch: 39/50 Step: 8750 Training Loss: 0.0798 Validation Loss: 1.2123\n",
            "Epoch: 39/50 Step: 8800 Training Loss: 0.1047 Validation Loss: 1.2365\n",
            "Epoch: 39/50 Step: 8850 Training Loss: 0.0365 Validation Loss: 1.2209\n",
            "Epoch: 40/50 Step: 8900 Training Loss: 0.1304 Validation Loss: 1.1935\n",
            "Epoch: 40/50 Step: 8950 Training Loss: 0.1251 Validation Loss: 1.1742\n",
            "Epoch: 40/50 Step: 9000 Training Loss: 0.0701 Validation Loss: 1.2321\n",
            "Epoch: 40/50 Step: 9050 Training Loss: 0.0954 Validation Loss: 1.1378\n",
            "Epoch: 40/50 Step: 9100 Training Loss: 0.2485 Validation Loss: 1.1740\n",
            "Epoch: 41/50 Step: 9150 Training Loss: 0.1050 Validation Loss: 1.1457\n",
            "Epoch: 41/50 Step: 9200 Training Loss: 0.0340 Validation Loss: 1.1502\n",
            "Epoch: 41/50 Step: 9250 Training Loss: 0.0296 Validation Loss: 1.1864\n",
            "Epoch: 41/50 Step: 9300 Training Loss: 0.0451 Validation Loss: 1.2322\n",
            "Epoch: 42/50 Step: 9350 Training Loss: 0.1154 Validation Loss: 1.2261\n",
            "Epoch: 42/50 Step: 9400 Training Loss: 0.1811 Validation Loss: 1.2077\n",
            "Epoch: 42/50 Step: 9450 Training Loss: 0.0957 Validation Loss: 1.2469\n",
            "Epoch: 42/50 Step: 9500 Training Loss: 0.0425 Validation Loss: 1.2673\n",
            "Epoch: 42/50 Step: 9550 Training Loss: 0.2675 Validation Loss: 1.2353\n",
            "Epoch: 43/50 Step: 9600 Training Loss: 0.0200 Validation Loss: 1.3082\n",
            "Epoch: 43/50 Step: 9650 Training Loss: 0.0355 Validation Loss: 1.2576\n",
            "Epoch: 43/50 Step: 9700 Training Loss: 0.0613 Validation Loss: 1.2692\n",
            "Epoch: 43/50 Step: 9750 Training Loss: 0.1140 Validation Loss: 1.2824\n",
            "Epoch: 43/50 Step: 9800 Training Loss: 0.0207 Validation Loss: 1.3072\n",
            "Epoch: 44/50 Step: 9850 Training Loss: 0.1233 Validation Loss: 1.2470\n",
            "Epoch: 44/50 Step: 9900 Training Loss: 0.0668 Validation Loss: 1.2358\n",
            "Epoch: 44/50 Step: 9950 Training Loss: 0.0480 Validation Loss: 1.2776\n",
            "Epoch: 44/50 Step: 10000 Training Loss: 0.1168 Validation Loss: 1.2784\n",
            "Epoch: 45/50 Step: 10050 Training Loss: 0.2323 Validation Loss: 1.2353\n",
            "Epoch: 45/50 Step: 10100 Training Loss: 0.0832 Validation Loss: 1.2805\n",
            "Epoch: 45/50 Step: 10150 Training Loss: 0.0171 Validation Loss: 1.2972\n",
            "Epoch: 45/50 Step: 10200 Training Loss: 0.0195 Validation Loss: 1.3017\n",
            "Epoch: 45/50 Step: 10250 Training Loss: 0.1408 Validation Loss: 1.2779\n",
            "Epoch: 46/50 Step: 10300 Training Loss: 0.0134 Validation Loss: 1.2856\n",
            "Epoch: 46/50 Step: 10350 Training Loss: 0.1022 Validation Loss: 1.2904\n",
            "Epoch: 46/50 Step: 10400 Training Loss: 0.0121 Validation Loss: 1.3077\n",
            "Epoch: 46/50 Step: 10450 Training Loss: 0.0434 Validation Loss: 1.3794\n",
            "Epoch: 47/50 Step: 10500 Training Loss: 0.0149 Validation Loss: 1.3522\n",
            "Epoch: 47/50 Step: 10550 Training Loss: 0.0133 Validation Loss: 1.3296\n",
            "Epoch: 47/50 Step: 10600 Training Loss: 0.0269 Validation Loss: 1.4003\n",
            "Epoch: 47/50 Step: 10650 Training Loss: 0.0526 Validation Loss: 1.3169\n",
            "Epoch: 47/50 Step: 10700 Training Loss: 0.1245 Validation Loss: 1.3409\n",
            "Epoch: 48/50 Step: 10750 Training Loss: 0.0317 Validation Loss: 1.3008\n",
            "Epoch: 48/50 Step: 10800 Training Loss: 0.0846 Validation Loss: 1.3350\n",
            "Epoch: 48/50 Step: 10850 Training Loss: 0.0076 Validation Loss: 1.3772\n",
            "Epoch: 48/50 Step: 10900 Training Loss: 0.0111 Validation Loss: 1.4046\n",
            "Epoch: 49/50 Step: 10950 Training Loss: 0.1100 Validation Loss: 1.3776\n",
            "Epoch: 49/50 Step: 11000 Training Loss: 0.0124 Validation Loss: 1.3620\n",
            "Epoch: 49/50 Step: 11050 Training Loss: 0.0091 Validation Loss: 1.3777\n",
            "Epoch: 49/50 Step: 11100 Training Loss: 0.0167 Validation Loss: 1.4012\n",
            "Epoch: 49/50 Step: 11150 Training Loss: 0.1365 Validation Loss: 1.3986\n",
            "Epoch: 50/50 Step: 11200 Training Loss: 0.2082 Validation Loss: 1.3555\n",
            "Epoch: 50/50 Step: 11250 Training Loss: 0.0156 Validation Loss: 1.3442\n",
            "Epoch: 50/50 Step: 11300 Training Loss: 0.0142 Validation Loss: 1.3267\n",
            "Epoch: 50/50 Step: 11350 Training Loss: 0.2127 Validation Loss: 1.3819\n",
            "Epoch: 50/50 Step: 11400 Training Loss: 2.4984 Validation Loss: 1.3813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating Accuracy and Loss on the Test Set."
      ],
      "metadata": {
        "id": "PQCNIwUeF2mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()\n",
        "test_losses = []\n",
        "num_correct = 0\n",
        "test_h = net.init_hidden(batch_size)\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "    test_h = tuple([each.data for each in test_h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    test_output, test_h = net(inputs)\n",
        "\n",
        "    # Get the actual batch size for the current batch\n",
        "    current_batch_size = labels.size(0)\n",
        "\n",
        "    # Slice the output to match the target size\n",
        "    test_output = test_output[:current_batch_size]\n",
        "\n",
        "    loss = criterion(test_output, labels.float())\n",
        "    test_losses.append(loss.item())\n",
        "\n",
        "    preds = torch.round(test_output.squeeze())\n",
        "    correct_tensor = preds.eq(labels.float().view_as(preds))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
        "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UH4OwJsHqUN",
        "outputId": "284a2a07-03dd-4563-c041-88b315ab8322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.4218\n",
            "Test Accuracy: 0.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for predicting custom review\n"
      ],
      "metadata": {
        "id": "H0BvHWTiF8kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net, review, seq_length=200):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    words = preprocess_text(review).split()\n",
        "    encoded_words = [vocab_to_int.get(word) for word in words if word in vocab_to_int]\n",
        "\n",
        "    if not encoded_words:\n",
        "        print(\"Your review must contain at least 1 word present in the vocabulary!\")\n",
        "        return None\n",
        "\n",
        "    padded_words = pad_text([encoded_words], seq_length)\n",
        "    padded_words = torch.from_numpy(padded_words).to(device)\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    # Initialize the hidden state with a batch size of 1\n",
        "    h = net.init_hidden(1)\n",
        "\n",
        "    output, h = net(padded_words)\n",
        "    # Get the prediction for the first (and only) item in the batch\n",
        "    # output.squeeze(0) will get the first element in the batch and squeeze it to remove unnecessary dimensions.\n",
        "    # output.squeeze(0)[0] will get the first element of this tensor, corresponding to the prediction for the review.\n",
        "    pred = torch.round(output.squeeze(0)[-1])\n",
        "\n",
        "    pred_value = pred.item() if isinstance(pred, torch.Tensor) else pred\n",
        "\n",
        "    msg = \"This is a positive review.\" if pred_value == 1 else \"This is a negative review.\"\n",
        "\n",
        "    return msg"
      ],
      "metadata": {
        "id": "pqPBYYblHtP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review1 = \"very good\"\n",
        "print(predict(net, review1))\n",
        "review2 = \"very bad movie\"\n",
        "print(predict(net, review2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDDKhasgKsXH",
        "outputId": "b8a1a5d0-0e38-45de-ed50-2bdc37090af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a positive review.\n",
            "This is a negative review.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review3 = \"this movie was horrible\"\n",
        "print(predict(net, review3))\n",
        "review4 = \"this movie was great\"\n",
        "print(predict(net, review4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phO9okEP4zNw",
        "outputId": "8cb3bbe3-3285-4fc4-bd0c-25727831fcd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a negative review.\n",
            "This is a positive review.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "torch.save(net.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "kFZmK1XTxjL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download model params\n",
        "from google.colab import files\n",
        "files.download('model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "JqXKrIvU4Lv7",
        "outputId": "c014ea96-13fb-45f9-a388-54033a6512ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_edf78fbf-5b8e-4db2-b799-87b6401456a7\", \"model.pth\", 320840128)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}